### paths
# input: &IBASE D:/Kevin/Machine Learning/FeedbackPEA/input/ # local
# input: &IBASE /kaggle/input/feedback-prize-effectiveness/ # kaggle
input: &IBASE /home/working/input/ # jarvis
train_csv: !join [*IBASE, train.csv]
test_csv: !join [*IBASE, test.csv]
sample_csv: !join [*IBASE, sample_submission.csv]
train_base: !join [*IBASE, train/]
test_base: !join [*IBASE, test/]
feedback_model: !join [*IBASE, feedback-bertopic/feedback_2021_topic_model]
feedback_csv: !join [*IBASE, feedback-bertopic/topic_model_feedback.csv]
feedmeta_csv: !join [*IBASE, feedback-bertopic/topic_model_metadata.csv]

# generated: &GBASE D:/Kevin/Machine Learning/FeedbackPEA/generated/ # local
# generated: &GBASE /kaggle/working/generated/ # kaggle
generated: &GBASE /home/working/generated/ # jarvis
train_folds: !join [*GBASE, train_folds.csv]
label_enc: !join [*GBASE, label_enc.pkl]

# weights_save: &WBASE D:/Kevin/Machine Learning/FeedbackPEA/weights/ # local
# weights_save: &WBASE /kaggle/working/weights/ # kaggle
weights_save: &WBASE /home/working/weights/ # jarvis

### model names and paths
model_name: &MODEL microsoft/deberta-v3-base
# model_name: &MODEL microsoft/deberta-base
# model_name: &MODEL distilbert-base-uncased
tokenizer_path: *MODEL

### kfold cv
folds: 5
fold_type: "stratified_group" # ["stratified", "group", "stratified_group"]
tr_folds: [4]

### model params
use_pretrained: false # [true, false]
pretrained_model_weights: !join [*IBASE, mlm/epoch_2/pytorch_model.bin]
num_classes: 3
output_hidden_states: false # [true, false]
pooler: "weighted" # [max, mean, mean_max, conv1d, attention, default, weighted]
multi_drop: true # [true, false]

### train params
text_lowercase: true # [true, false]
epochs: 3
n_accumulate: 1
max_length: 512
train_bs: 64
valid_bs: 64

### optimizer
optimizer: "adamw" # [adamw, lamb, radam]
model_lr: 0.00004
pooler_lr: 0.00004
eps: 0.000001
wd: 0.01
b1: 0.9
b2: 0.999
max_grad_norm: 1000

### scheduler
scheduler: cosine # ['linear', 'cosine', 'cosine_with_restarts', 'polynomial', 'constant', 'constant_with_warmup', CosineAnnealingLR, CosineAnnealingWarmRestarts, None]
warmup_steps: 200
cycles: 0.5
T_max: 500
min_lr: 0.000001

### criterion
criterion: "ce" # [smooth_ce, ce, btll]
smoothing: 0.2

### hardware
cpu: false # [true, false]
tpu: false # [true, false]
amp: "fp16" # ["no", "fp16", "bf16"]

### seed
seed: 42

### logging
freq: 5
valid_freq_per_epoch: 3

### debug
debug: False

### kaggle save config
title: exp-no-12

### mlm pretrain config
# mlm_src: &MLM D:/Kevin/Machine Learning/FeedbackPEA/src/mlm/ # local
# mlm_src: &MLM /kaggle/working/src/mlm/ # kaggle
mlm_src: &MLM /home/working/src/mlm/ # jarvis
mlm_gen: !join [*MLM, generated/]

mlm_f1_train_csv: !join [*MLM, input/feedback-prize-2021/train.csv]
mlm_f2_train_csv: !join [*MLM, input/feedback-prize-effectiveness/train.csv]
mlm_f1_train_base: !join [*MLM, input/feedback-prize-2021/train/]
mlm_f2_train_base: !join [*MLM, input/feedback-prize-2021/train/]

mlm_dataset_line_by_line: true # [true, false]
mlm_test_split: 0.1
mlm_bs: 8
mlm_debug: false # [true, false]